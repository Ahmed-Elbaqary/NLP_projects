{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72dae01c-7c3a-44dd-b23c-204f849520cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../data/processed/00_train_df.pkl\"\n",
    "VAL_PATH = \"../data/processed/00_validation_df.pkl\"\n",
    "TEST_PATH = \"../data/processed/00_test_df.pkl\"\n",
    "\n",
    "TRAIN_FEATURES_BOW = \"../data/processed/01_train_features_BOW.pkl\"\n",
    "VAL_FEATURES_BOW = \"../data/processed/01_validation_features_BOW.pkl\"\n",
    "TEST_FEATURES_BOW = \"../data/processed/01_test_features_BOW.pkl\"\n",
    "\n",
    "TRAIN_FEATURES_TFIDF = \"../data/processed/01_train_features_TFIDF.pkl\"\n",
    "VAL_FEATURES_TFIDF = \"../data/processed/01_validation_features_TFIDF.pkl\"\n",
    "TEST_FEATURES_TFIDF = \"../data/processed/01_test_features_TFIDF.pkl\"\n",
    "\n",
    "\n",
    "TRAIN_TARGET_EXPORT = \"../data/processed/01_train_target.pkl\"\n",
    "VAL_TARGET_EXPORT = \"../data/processed/01_validation_target.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614aed3c-0a87-4453-bd9c-424585302cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5f3ed-3bb2-4ab8-98e1-08e0a09ffafa",
   "metadata": {},
   "source": [
    "___\n",
    "## Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36846c08-b865-45a5-93ab-f1b29a834037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality(ground_truth, predictions, metric_function, model_name, Average='macro'):\n",
    "    \"\"\"\n",
    "    Calculate the quality of the model according to different metric scores\n",
    "    Input:\n",
    "        ground_truth: from real observed data\n",
    "        predictions: the predicted values from the model\n",
    "        metric_function: the metric score funcrion used to measure performance\n",
    "    Output:\n",
    "        A dict of all scores for the given inputs\n",
    "    \"\"\"\n",
    "    quality_score = {}\n",
    "    if metric_function.__name__ == 'accuracy_score':\n",
    "        quality_score[model_name] = round(metric_function(ground_truth, predictions), 3)\n",
    "    else:\n",
    "        quality_score[model_name] = round(metric_function(ground_truth, predictions, average=Average), 3)\n",
    "\n",
    "    quality_score = pd.Series(quality_score.values(), index=quality_score.keys())\n",
    "    \n",
    "    return quality_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8221b8c8-3c63-40af-b00b-e3e6a0d1d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_list, metrics_list, model_name, score_average='macro'):\n",
    "    \"\"\"\n",
    "    Get the scores of the model to better understand its performance\n",
    "    Input:\n",
    "        data_list: a list of all data that we evaluate model upon, train and test data.\n",
    "                typical input : [[X_train, y_train, 'train'], [X_test, y_test, 'test']]\n",
    "        scores_list: a list of all metrics used in the evaluation. \n",
    "                typical input : [accuracy_score, precision_score, recall_score, f1_score]\n",
    "        model_name: a string input used as the index for score dataframe.\n",
    "    Output:\n",
    "        scores: a dataframe of evaluation based on data.\n",
    "        general_error: the generalized error that would be used for logging in mlflow\n",
    "    \"\"\"\n",
    "    predicts = []\n",
    "    scores = []\n",
    "    for [X,y,stage] in data_list:\n",
    "\n",
    "        probas = model.predict(X)\n",
    "        predictions = pd.DataFrame(probas)\n",
    "        predicts.append(predictions)\n",
    "\n",
    "        result = {score.__name__:calculate_quality(y, predictions, score, f\"{model_name}_{stage}\", score_average)\n",
    "                        for score in metrics_list}\n",
    "\n",
    "        result = pd.concat(result, axis=1)\n",
    "        scores.append(result)\n",
    "    scores = pd.concat(scores)\n",
    "    return scores, predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b39f260d-e81d-4204-9dc7-d22168ec71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_words_for_tag(classifier, tag, tags_classes, index_to_words):\n",
    "    \"\"\"\n",
    "        classifier: trained classifier\n",
    "        tag: particular tag\n",
    "        tags_classes: a list of classes names from MultiLabelBinarizer\n",
    "        index_to_words: index_to_words transformation\n",
    "        all_words: all words in the dictionary\n",
    "        \n",
    "        return nothing, just print top 5 positive and top 5 negative words for current tag\n",
    "    \"\"\"\n",
    "    print('Tag:\\t{}'.format(tag))\n",
    "    \n",
    "    coef = classifier.coef_[tags_classes.index(tag)]\n",
    "    \n",
    "    top_positive_words = [index_to_words[idx] for idx in coef.argsort()[-1:-10:-1]]# top-5 words sorted by the coefficiens.\n",
    "    top_negative_words = [index_to_words[idx] for idx in coef.argsort()[:10]]# bottom-5 words  sorted by the coefficients.\n",
    "    print('Top positive words:\\t{}'.format(', '.join(top_positive_words)))\n",
    "    print('Top negative words:\\t{}\\n'.format(', '.join(top_negative_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fbbfd9-0517-4e1a-b1e1-6b89e80b44b5",
   "metadata": {},
   "source": [
    "___\n",
    "## Read data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f24f48-fd9f-4931-ad8f-b475d273bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(TRAIN_PATH)\n",
    "val_df = pd.read_pickle(VAL_PATH)\n",
    "test_df = pd.read_pickle(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab315be-b2d8-4a38-8676-f3650b286d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BOW features\n",
    "with open(TRAIN_FEATURES_BOW, 'rb') as handle:\n",
    "    X_train_BOW = pickle.load(handle)\n",
    "\n",
    "with open(VAL_FEATURES_BOW, 'rb') as handle:\n",
    "    X_val_BOW = pickle.load(handle)\n",
    "\n",
    "with open(TEST_FEATURES_BOW, 'rb') as handle:\n",
    "    X_test_BOW = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63a53efb-00b6-4c69-97ae-ddba5ef70f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TF-IDF features\n",
    "with open(TRAIN_FEATURES_TFIDF, 'rb') as handle:\n",
    "    X_train_tfidf = pickle.load(handle)\n",
    "\n",
    "with open(VAL_FEATURES_TFIDF, 'rb') as handle:\n",
    "    X_val_tfidf = pickle.load(handle)\n",
    "\n",
    "with open(TEST_FEATURES_TFIDF, 'rb') as handle:\n",
    "    X_test_tfidf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46e6b64c-70f2-4a5c-88e8-144b11f54a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MultiLabelBinarizer target\n",
    "with open(TRAIN_TARGET_EXPORT, 'rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "\n",
    "with open(VAL_TARGET_EXPORT, 'rb') as handle:\n",
    "    y_val = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0148cb0e-8f85-4a4d-80f3-778394025be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MLB\n",
    "with open('../data/objects/mlb.pkl', 'rb') as handle:\n",
    "    mlb = pickle.load(handle)\n",
    "    \n",
    "# Load the BOW_vectorizer\n",
    "with open('../data/objects/BOW_vectorizer.pkl', 'rb') as handle:\n",
    "    BOW_vectorizer = pickle.load(handle)\n",
    "    \n",
    "# Load the tfidf_vectorizer\n",
    "with open('../data/objects/tfidf_vectorizer.pkl', 'rb') as handle:\n",
    "    tfidf_vectorizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3acab16-e834-4c8d-9e89-bd5586a7cb75",
   "metadata": {},
   "source": [
    "___\n",
    "## Construct the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fed7b876-dab8-41a8-aeba-f7d32f887a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "lor = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc9963-0e4e-40c2-b53a-39ded648bdb1",
   "metadata": {},
   "source": [
    "___\n",
    "## Train model with Bag_Of_Words representaiton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa22638a-f891-4270-bde8-e01c521561c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('onevsrestclassifier',\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(solver='liblinear')))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_clf = make_pipeline(OneVsRestClassifier(lor))\n",
    "BOW_clf.fit(X_train_BOW, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f640b343-bb28-4b57-8f60-4abf819e4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and metrics that would be used to evaluate the model\n",
    "data_list = [[X_train_BOW, y_train, 'train'], [X_val_BOW, y_val, 'test']]\n",
    "metrics_list = [accuracy_score, precision_score, recall_score, f1_score] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56412690-cbf7-4210-9bad-ee72f8434974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_train</th>\n",
       "      <td>0.380</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_test</th>\n",
       "      <td>0.329</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy_score  precision_score  recall_score  \\\n",
       "LogisticRegression_train           0.380            0.888         0.565   \n",
       "LogisticRegression_test            0.329            0.843         0.525   \n",
       "\n",
       "                          f1_score  \n",
       "LogisticRegression_train     0.691  \n",
       "LogisticRegression_test      0.648  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use evaluate_model function to run evaluations\n",
    "scores_BOW, predicts_BOW = evaluate_model(BOW_clf, data_list, metrics_list, \"LogisticRegression\", 'micro')\n",
    "scores_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a75a6ac1-8d77-48a6-912c-78e12d54a0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_train</th>\n",
       "      <td>0.380</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_test</th>\n",
       "      <td>0.329</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy_score  precision_score  recall_score  \\\n",
       "LogisticRegression_train           0.380            0.818         0.458   \n",
       "LogisticRegression_test            0.329            0.694         0.401   \n",
       "\n",
       "                          f1_score  \n",
       "LogisticRegression_train     0.565  \n",
       "LogisticRegression_test      0.494  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use evaluate_model function to run evaluations\n",
    "scores_BOW, predicts_BOW = evaluate_model(BOW_clf, data_list, metrics_list, \"LogisticRegression\", 'macro')\n",
    "scores_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1783a9da-1a1c-4333-930a-8a964b70f574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_train</th>\n",
       "      <td>0.380</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_test</th>\n",
       "      <td>0.329</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy_score  precision_score  recall_score  \\\n",
       "LogisticRegression_train           0.380            0.864         0.565   \n",
       "LogisticRegression_test            0.329            0.801         0.525   \n",
       "\n",
       "                          f1_score  \n",
       "LogisticRegression_train     0.669  \n",
       "LogisticRegression_test      0.624  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use evaluate_model function to run evaluations\n",
    "scores_BOW, predicts_BOW = evaluate_model(BOW_clf, data_list, metrics_list, \"LogisticRegression\", 'weighted')\n",
    "scores_BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0a189f-19eb-4238-b9c5-9d43c536a08e",
   "metadata": {},
   "source": [
    "**Run some random samples to test the classifier** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0db3593-df09-4cd0-a7b0-56fead68b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the invers of the y_val predictions to find the predicted class\n",
    "y_val_inv_BOW = mlb.inverse_transform(predicts_BOW[1].values) # (predicts) has the predictions of both X_train_BOW and X_val_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58229772-068c-4f9a-9195-3adcbaa38b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  way create bean class json response\n",
      "True labels:  ['java', 'json']\n",
      "Predicted labels:  ('java', 'json')\n",
      "\n",
      "title:  windowonload fired\n",
      "True labels:  ['javascript']\n",
      "Predicted labels:  ('javascript',)\n",
      "\n",
      "title:  format lld expects type long long int argument 4 type int64_t\n",
      "True labels:  ['c', 'linux']\n",
      "Predicted labels:  ()\n",
      "\n",
      "title:  would pass data child view parent view using iphone sdk\n",
      "True labels:  ['iphone', 'objective-c']\n",
      "Predicted labels:  ('iphone', 'objective-c')\n",
      "\n",
      "title:  welcome back youve already connected app via google+ signin\n",
      "True labels:  ['javascript']\n",
      "Predicted labels:  ()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = np.random.randint(1, val_df.shape[0], 5)\n",
    "for i in samples:\n",
    "    print(\"title: \", val_df['title'][i])\n",
    "    print('True labels: ', val_df['tags'][i])\n",
    "    print(\"Predicted labels: \", y_val_inv_BOW[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79794ab9-7e91-49a4-89e0-463a0ba9dcd1",
   "metadata": {},
   "source": [
    "___\n",
    "## Train model with TF-IDF representaiton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0210d0c-9702-4e7d-8dc1-44056033bf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('onevsrestclassifier',\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(solver='liblinear')))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_clf = make_pipeline(OneVsRestClassifier(lor))\n",
    "tfidf_clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13fc7e25-a1f6-4afe-8600-dd6e46b9e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and metrics that would be used to evaluate the model\n",
    "data_list = [[X_train_tfidf, y_train, 'train'], [X_val_tfidf, y_val, 'test']]\n",
    "metrics_list = [accuracy_score, precision_score, recall_score, f1_score] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38c443e0-3a0a-4bca-bd7e-b62294c347a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_train</th>\n",
       "      <td>0.362</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_test</th>\n",
       "      <td>0.334</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy_score  precision_score  recall_score  \\\n",
       "LogisticRegression_train           0.362            0.920         0.522   \n",
       "LogisticRegression_test            0.334            0.893         0.501   \n",
       "\n",
       "                          f1_score  \n",
       "LogisticRegression_train     0.666  \n",
       "LogisticRegression_test      0.642  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use evaluate_model function to run evaluations\n",
    "scores_tfidf, predicts_tfidf = evaluate_model(tfidf_clf, data_list, metrics_list, \"LogisticRegression\", 'micro')\n",
    "scores_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a3e9de1-1102-42aa-aff4-c8311b0c4506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_train</th>\n",
       "      <td>0.362</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_test</th>\n",
       "      <td>0.334</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy_score  precision_score  recall_score  \\\n",
       "LogisticRegression_train           0.362            0.797         0.366   \n",
       "LogisticRegression_test            0.334            0.734         0.340   \n",
       "\n",
       "                          f1_score  \n",
       "LogisticRegression_train     0.478  \n",
       "LogisticRegression_test      0.446  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use evaluate_model function to run evaluations\n",
    "scores_tfidf, predicts_tfidf = evaluate_model(tfidf_clf, data_list, metrics_list, \"LogisticRegression\", 'macro')\n",
    "scores_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eb0d5d7-8895-4f28-ae28-9c401ea10b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_train</th>\n",
       "      <td>0.362</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_test</th>\n",
       "      <td>0.334</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy_score  precision_score  recall_score  \\\n",
       "LogisticRegression_train           0.362            0.883         0.522   \n",
       "LogisticRegression_test            0.334            0.845         0.501   \n",
       "\n",
       "                          f1_score  \n",
       "LogisticRegression_train     0.639  \n",
       "LogisticRegression_test      0.614  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use evaluate_model function to run evaluations\n",
    "scores_tfidf, predicts_tfidf = evaluate_model(tfidf_clf, data_list, metrics_list, \"LogisticRegression\", 'weighted')\n",
    "scores_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3917b-cabc-4ebe-9fd6-bb82884fec6b",
   "metadata": {},
   "source": [
    "**Run some random samples to test the classifier** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6472eaa4-39b8-4176-8489-af280746fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the invers of the y_val predictions to find the predicted class\n",
    "y_val_inv_tfidf = mlb.inverse_transform(predicts_tfidf[1].values) # (predicts) has the predictions of both X_train_BOW and X_val_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14100915-a28b-4a8e-ae4a-26850dfd4e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  g++ compiling c++11 using wpedantic option option disable warning unnamed structs\n",
      "True labels:  ['c++']\n",
      "Predicted labels:  ('c++',)\n",
      "\n",
      "title:  itext merge documents acrofields\n",
      "True labels:  ['java']\n",
      "Predicted labels:  ('java',)\n",
      "\n",
      "title:  get current index combobox\n",
      "True labels:  ['c#', '.net', 'winforms']\n",
      "Predicted labels:  ()\n",
      "\n",
      "title:  validate uitextview allow special characters like\n",
      "True labels:  ['iphone', 'objective-c']\n",
      "Predicted labels:  ()\n",
      "\n",
      "title:  angulartojson scopeobject jsonstringify scopeobject return null\n",
      "True labels:  ['javascript', 'json', 'angularjs']\n",
      "Predicted labels:  ()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = np.random.randint(1, val_df.shape[0], 5)\n",
    "for i in samples:\n",
    "    print(\"title: \", val_df['title'][i])\n",
    "    print('True labels: ', val_df['tags'][i])\n",
    "    print(\"Predicted labels: \", y_val_inv_tfidf[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a0fa3-5218-4456-8a09-75b937a9f69d",
   "metadata": {},
   "source": [
    "___\n",
    "## Analysis of most important features for each class:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f277175-6a4d-4c50-8d10-7f9f29b8ee5b",
   "metadata": {},
   "source": [
    "**1. BOW classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7872aad7-de22-4108-8b0b-1e3e76ce7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_reversed_vocab = {i:word for word, i in tfidf_vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b8929d1-a989-4419-865d-59edf4954209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag:\tc\n",
      "Top positive words:\tbranching, best, back button, breakpoints, content div, c# problem, connections, count elements, data points\n",
      "Top negative words:\t#1, #ifdef, + c#, 2 decimal, 12, #2, 0, #pragma, 10 seconds, 1 mysqli\n",
      "\n",
      "Tag:\tc++\n",
      "Top positive words:\tadd field, angularjs form, basic net, checkbox click, based application, applied, color value, call member, cast\n",
      "Top negative words:\t#1, #ifdef, + c#, 10 seconds, 2 decimal, 12, 0, #pragma, 2d numpy, 192\n",
      "\n",
      "Tag:\tlinux\n",
      "Top positive words:\taccess array, c using, data points, code c++, allowed memory, content html, blade, c# problem, descriptors\n",
      "Top negative words:\t#ifdef, + c#, 1 mysqli, 301, + php, + ajax, + sql, 1 0, 1404, browser windows\n",
      "\n",
      "Tag:\tjavascript\n",
      "Top positive words:\t#ifdef, ascending order, 3 seconds, abc, access session, auto complete, 2d numpy, associated, background color\n",
      "Top negative words:\t24 hours, #pragma, 2 decimal, 22, access file, 3 razor, 10 seconds, alt, ado, ajax javascript\n",
      "\n",
      "Tag:\tpython\n",
      "Top positive words:\t#pragma, activity, absolute path, also, add trailing, anchor, app crash, array numbers, body class\n",
      "Top negative words:\t#1, #2, 2 decimal, 22, 2010 c++, 0, 1 mysqli, 3 razor, + c#, 4 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_words_for_tag(BOW_clf.named_steps['onevsrestclassifier'], 'c', mlb.classes, BOW_reversed_vocab)\n",
    "print_words_for_tag(BOW_clf.named_steps['onevsrestclassifier'], 'c++', mlb.classes, BOW_reversed_vocab)\n",
    "print_words_for_tag(BOW_clf.named_steps['onevsrestclassifier'], 'linux', mlb.classes, BOW_reversed_vocab)\n",
    "print_words_for_tag(BOW_clf.named_steps['onevsrestclassifier'], 'javascript', mlb.classes, BOW_reversed_vocab)\n",
    "print_words_for_tag(BOW_clf.named_steps['onevsrestclassifier'], 'python', mlb.classes, BOW_reversed_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c96e239-f72e-4630-afd4-1f42e86cbe93",
   "metadata": {},
   "source": [
    "**2. TF-IDF classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55bd6d6c-a50d-404d-9848-221e2708429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_reversed_vocab = {i:word for word, i in tfidf_vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b250f763-68b9-485f-9039-76acca73555d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag:\tc\n",
      "Top positive words:\tc, malloc, scanf, printf, gcc, pointer, linux, kernel, struct\n",
      "Top negative words:\tjava, php, python, javascript, c#, objective c, objective, jquery, ruby, swift\n",
      "\n",
      "Tag:\tc++\n",
      "Top positive words:\tc++, qt, boost, mfc, opencv, stl, c++11, stdstring, boostasio\n",
      "Top negative words:\tjava, php, python, javascript, c#, jquery, r, ruby, swift, rails\n",
      "\n",
      "Tag:\tlinux\n",
      "Top positive words:\tlinux, ubuntu, c, address, signal, shared, unix, fork, process\n",
      "Top negative words:\tjavascript, c#, jquery, array, method, aspnet, image, android, page, string\n",
      "\n",
      "Tag:\tjavascript\n",
      "Top positive words:\tjavascript, jquery, js, angularjs, nodejs, angular, div, extjs, typescript\n",
      "Top negative words:\tpython, c#, java, php, c++, django, swift, wpf, c, rails\n",
      "\n",
      "Tag:\tpython\n",
      "Top positive words:\tpython, pandas, numpy, matplotlib, flask, tkinter, sqlalchemy, django, beautifulsoup\n",
      "Top negative words:\tphp, java, c#, javascript, jquery, c++, r, rails, c, swift\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_words_for_tag(tfidf_clf.named_steps['onevsrestclassifier'], 'c', mlb.classes, tfidf_reversed_vocab)\n",
    "print_words_for_tag(tfidf_clf.named_steps['onevsrestclassifier'], 'c++', mlb.classes, tfidf_reversed_vocab)\n",
    "print_words_for_tag(tfidf_clf.named_steps['onevsrestclassifier'], 'linux', mlb.classes, tfidf_reversed_vocab)\n",
    "print_words_for_tag(tfidf_clf.named_steps['onevsrestclassifier'], 'javascript', mlb.classes, tfidf_reversed_vocab)\n",
    "print_words_for_tag(tfidf_clf.named_steps['onevsrestclassifier'], 'python', mlb.classes, tfidf_reversed_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a951a010-95a9-4b5b-b365-076ba8a85306",
   "metadata": {},
   "source": [
    "___\n",
    "**As we can see; the classifier trained on the tfidf representation is more robust and accurate than the classifier trained on the BOW document representation; and so we can notice this clearly when we examine the most positive and negative words that affects our model. We can see that the words are somehow relevant to the tag or class it predicts in the tfidf classifier and so we can see the huge effect of this representation than the traditional BOW**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
