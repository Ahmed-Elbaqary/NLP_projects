{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25fccc3f-494c-446c-af64-2f8d9d4aadb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_TRAIN = \"../data/processed/train_final.pkl\"\n",
    "FINAL_TEST = \"../data/processed/test_final.pkl\"\n",
    "\n",
    "FEATURES_DICT = \"../data/objects/features.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3a3b791-7819-4ded-a8b1-5d2efed2e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f477c3bf-a7d7-4796-8505-b83ab7826f2d",
   "metadata": {},
   "source": [
    "___\n",
    "## Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "827cad6e-85a8-4cd1-9f94-65db4d117f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs=features):\n",
    "    words = process_tweet(tweet)\n",
    "    tweet_array = np.array([[1.0, 0.0, 0.0]])\n",
    "    \n",
    "    for word in words:\n",
    "        tweet_array[0, 1] += freqs.get((word, 1.0), 0)\n",
    "        tweet_array[0, 2] += freqs.get((word, 0.0), 0)\n",
    "    \n",
    "    assert(tweet_array.shape == (1, 3))\n",
    "    return tweet_array#.reshape(3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18cf3a4b-0602-432e-a5fb-b81eece6e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality(ground_truth, predictions, metric_function, model_name):\n",
    "    \"\"\"\n",
    "    Calculate the quality of the model according to different metric scores\n",
    "    Input:\n",
    "        ground_truth: from real observed data\n",
    "        predictions: the predicted values from the model\n",
    "        metric_function: the metric score funcrion used to measure performance\n",
    "    Output:\n",
    "        A dict of all scores for the given inputs\n",
    "    \"\"\"\n",
    "    quality_score = {}\n",
    "    quality_score[model_name] = round(metric_function(ground_truth, predictions), 3)\n",
    "\n",
    "    quality_score = pd.Series(quality_score.values(), index=quality_score.keys())\n",
    "    \n",
    "    return quality_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "061eb2b7-fe1c-4d10-adff-a764e1e8e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_list, metrics_list, model_name):\n",
    "    \"\"\"\n",
    "    Get the scores of the model to better understand its performance\n",
    "    Input:\n",
    "        data_list: a list of all data that we evaluate model upon, train and test data.\n",
    "                typical input : [[X_train, y_train, 'train'], [X_test, y_test, 'test']]\n",
    "        scores_list: a list of all metrics used in the evaluation. \n",
    "                typical input : [accuracy_score, precision_score, recall_score, f1_score]\n",
    "        model_name: a string input used as the index for score dataframe.\n",
    "    Output:\n",
    "        scores: a dataframe of evaluation based on data.\n",
    "        general_error: the generalized error that would be used for logging in mlflow\n",
    "    \"\"\"\n",
    "    predicts = []\n",
    "    scores = []\n",
    "    for [X,y,stage] in data_list:\n",
    "\n",
    "        probas = model.predict(X)\n",
    "        predictions = pd.DataFrame(probas)\n",
    "        predicts.append(predictions)\n",
    "\n",
    "        result = {score.__name__:calculate_quality(y, predictions, score, f\"{model_name}_{stage}\")\n",
    "                        for score in metrics_list}\n",
    "\n",
    "        result = pd.concat(result, axis=1)\n",
    "        scores.append(result)\n",
    "    scores = pd.concat(scores)\n",
    "    return scores, predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd1b73-712c-4f1b-b8a0-4cc667803579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweet(tweet, clf):\n",
    "    tweet = process_tweet(tweet)\n",
    "    features = extract_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab42094-5e81-4232-bddb-a9c7387cde73",
   "metadata": {},
   "source": [
    "___\n",
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6684a41-fc5c-48df-8efe-11b5ff5c9550",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FEATURES_DICT, 'rb') as handle:\n",
    "    features = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c26515d-9712-4a79-a7f0-687849ba6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(FINAL_TRAIN)\n",
    "test_df = pd.read_pickle(FINAL_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5b336ac-7466-4979-9508-4083c63e7880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4603.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2954.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7285</th>\n",
       "      <td>1.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>7616.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736</th>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3781.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4123.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bias     pos     neg  sentiment\n",
       "340    1.0  4603.0   697.0        1.0\n",
       "629    1.0  2954.0    94.0        1.0\n",
       "7285   1.0   223.0  7616.0        0.0\n",
       "4736   1.0    68.0  3781.0        0.0\n",
       "547    1.0  4123.0   599.0        1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f1aa02-e916-4b6f-a774-92f62a7d4d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(['sentiment'], axis=1)\n",
    "X_test = test_df.drop(['sentiment'], axis=1)\n",
    "\n",
    "y_train = train_df['sentiment']\n",
    "y_test = test_df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb815a54-997a-4942-bbce-804e820c71dc",
   "metadata": {},
   "source": [
    "___\n",
    "## Building model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e937cf1c-6137-40dd-a105-755d22826179",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(MinMaxScaler(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24876ba8-7df0-4ebb-9b6a-b6a8fbfb88a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c36bb397-d943-4574-a081-701599b006fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and metrics that would be used to evaluate the model\n",
    "data_list = [[X_train, y_train, 'train'], [X_test, y_test, 'test']]\n",
    "metrics_list = [accuracy_score, precision_score, recall_score, f1_score] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "734b9a5a-c371-48bf-9708-9f92db67cb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_train</th>\n",
       "      <td>0.943</td>\n",
       "      <td>0.897</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_test</th>\n",
       "      <td>0.941</td>\n",
       "      <td>0.894</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          accuracy_score  precision_score  recall_score  \\\n",
       "LogisticRegression_train           0.943            0.897           1.0   \n",
       "LogisticRegression_test            0.941            0.894           1.0   \n",
       "\n",
       "                          f1_score  \n",
       "LogisticRegression_train     0.946  \n",
       "LogisticRegression_test      0.944  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use evaluate_model function to run evaluations\n",
    "scores, predicts = evaluate_model(clf, data_list, metrics_list, \"LogisticRegression\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639877d8-5ee9-49cc-93db-0b2f54b3b3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
