{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d347d690-c775-4b73-88b1-571a75caf32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"predict_question_tags_stackoverflow/data/processed/00_train_df.pkl\"\n",
    "VAL_PATH = \"predict_question_tags_stackoverflow/data/processed/00_validation_df.pkl\"\n",
    "TEST_PATH = \"predict_question_tags_stackoverflow/data/processed/00_test_df.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b219505a-947c-48b1-b19b-cdfddb70c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.display.max_columns = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b7fb4ab-9ae2-41d2-9392-a226cc4d500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(TRAIN_PATH)\n",
    "val_df = pd.read_pickle(VAL_PATH)\n",
    "test_df = pd.read_pickle(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b273e4-546a-46db-b4d2-705eed06b89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>draw stacked dotplot r</td>\n",
       "      <td>[r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mysql select records datetime field less specified value</td>\n",
       "      <td>[php, mysql]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>terminate windows phone 81 app</td>\n",
       "      <td>[c#]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get current time specific country via jquery</td>\n",
       "      <td>[javascript, jquery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>configuring tomcat use ssl</td>\n",
       "      <td>[java]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      title  \\\n",
       "0                                    draw stacked dotplot r   \n",
       "1  mysql select records datetime field less specified value   \n",
       "2                            terminate windows phone 81 app   \n",
       "3              get current time specific country via jquery   \n",
       "4                                configuring tomcat use ssl   \n",
       "\n",
       "                   tags  \n",
       "0                   [r]  \n",
       "1          [php, mysql]  \n",
       "2                  [c#]  \n",
       "3  [javascript, jquery]  \n",
       "4                [java]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5b8b5-1b25-4c60-8c2e-d2d2081bb64a",
   "metadata": {},
   "source": [
    "## First step: count tag/word frequency:\n",
    "**At the first step we will get the count of each word and tag so we can sort them and find the best vectors we can classify upon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2d4c064-7528-4cac-a14d-c98cb9af5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "tags_counts = Counter() # counter for tags\n",
    "words_counts = Counter() # counter for words in titles\n",
    "#We can use ============> tags_dict = defaultdict(int)\n",
    "\n",
    "for tags in train_df['tags']:\n",
    "    for tag in tags:\n",
    "        tags_counts[tag] += 1\n",
    "\n",
    "for words in train_df['title']:\n",
    "    for word in words.split():\n",
    "        words_counts[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98fa393-4855-4c55-8946-2a50b0ecb3e1",
   "metadata": {},
   "source": [
    "**Now sort to construct our bag of words BOW we need to transform each sentense into  a vector of features** \\\n",
    "so we need first to identify our words as features and so we will sort the most common words to N number we specify and so we will loop through sentense and check wether this word exist in the features or not, if it exists it will get 1 (or n number of existing in the sentense)\n",
    "\n",
    "so for example: features are \\['hi', 'you', 'me', 'are'\\]\n",
    "\n",
    "sentense is \"hi how are you\" \n",
    "\n",
    "so as a vector it would look like \\[1, 1, 0, 1\\]\n",
    "\n",
    "and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c633aef0-975a-4905-a797-c76a0dc8ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first set your features and sort them with their indices\n",
    "DICT_SIZE = 5000\n",
    "\n",
    "#SORT_BY_FREQ = sorted(words_counts.keys(), key=lambda x: words_counts[x], reverse=True)[:DICT_SIZE] # a sorted list of DICT_SIZE words according to frequency\n",
    "SORT_BY_FREQ = [x[0] for x in words_counts.most_common(DICT_SIZE)]\n",
    "\n",
    "WORDS_TO_INDEX = {word: i for i, word in enumerate(SORT_BY_FREQ)} # Putting each word with its index in a dict so as to find its place in the word vector\n",
    "#ALL_WORDS = WORDS_TO_INDEX.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ad9dc181-c30d-4958-b5ae-514d626fc638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text, words_index_dict, dict_size):\n",
    "    result_vector = np.zeros(dict_size)\n",
    "    for word in text.split():\n",
    "        if word in words_index_dict:\n",
    "            result_vector[words_index_dict[word]] += 1\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36724a5-92ac-465f-9b16-65341f0cea2e",
   "metadata": {},
   "source": [
    "**Now after constructing features and function we will transform our text into feature form for all train, validat and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4144d857-fc3e-4678-bbd9-5d4eea7596ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "X_train_BOW = sparse.vstack([sparse.csr_matrix(bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in train_df['title']])\n",
    "X_val_BOW = sparse.vstack([sparse.csr_matrix(bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in val_df['title']])\n",
    "X_test_BOW = sparse.vstack([sparse.csr_matrix(bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in test_df['title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a0ca1cde-b7af-4789-8355-0a404d49a3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 31)\t1.0\n",
      "  (0, 34)\t1.0\n",
      "  (0, 94)\t1.0\n",
      "  (0, 236)\t1.0\n",
      "  (0, 395)\t1.0\n",
      "  (0, 518)\t1.0\n",
      "  (0, 1143)\t1.0\n",
      "  (0, 2394)\t1.0\n",
      "  (0, 2508)\t1.0\n",
      "  (0, 4852)\t1.0\n",
      "(100000, 5000)\n",
      "['using', 'php', 'java', 'file', 'javascript', 'error', 'get', 'c#', 'python', 'string']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_BOW[5])\n",
    "print(X_train_BOW.shape)\n",
    "print(SORT_BY_FREQ[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b023e6-1897-4454-8495-2b667fe8ebb7",
   "metadata": {},
   "source": [
    "___\n",
    "## We can use the CountVectorizer class to build our BOW as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1da32078-072b-4a7c-818b-4d6df8346266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(vocabulary=SORT_BY_FREQ)\n",
    "X = vectorizer.fit_transform(train_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "70eb4bdb-ce16-4e1a-94e3-9112088ecdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 31)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 94)\t1\n",
      "  (0, 236)\t1\n",
      "  (0, 395)\t1\n",
      "  (0, 518)\t1\n",
      "  (0, 1143)\t1\n",
      "  (0, 2394)\t1\n",
      "  (0, 2508)\t1\n",
      "  (0, 4852)\t1\n",
      "(100000, 5000)\n",
      "['using' 'php' 'java' 'file' 'javascript' 'error' 'get' 'c#' 'python'\n",
      " 'string']\n"
     ]
    }
   ],
   "source": [
    "print(X[5])\n",
    "print(X.shape)\n",
    "print(vectorizer.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e9c7db-95c5-41fa-9aa3-1784be59d8c3",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "The second approach extends the bag-of-words framework by taking into account total frequencies of words in the corpora. It helps to penalize too frequent words and provide better features space.\n",
    "\n",
    "Implement function tfidf_features using class TfidfVectorizer from scikit-learn. Use train corpus to train a vectorizer. Don't forget to take a look into the arguments that you can pass to it. We suggest that you filter out too rare words (occur less than in 5 titles) and too frequent words (occur more than in 90% of the titles). Also, use bigrams along with unigrams in your vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0e979f4c-3661-41bb-9bfd-b208de0ed26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d5ed9fcb-38cc-4fb1-b7e2-f2a5656b65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.9,\n",
    "                                   ngram_range=(1, 2),\n",
    "                                   token_pattern='(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d327d4b9-6ce8-4fa4-a76e-99a7e85c8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['title'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(val_df['title'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b03f37c0-44c3-4d57-a25b-48d9fdf7e3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 461)\t0.32939124453958957\n",
      "  (0, 9091)\t0.3655867713235215\n",
      "  (0, 17370)\t0.3626593411993441\n",
      "  (0, 16059)\t0.28363456686039207\n",
      "  (0, 2590)\t0.3232152261847885\n",
      "  (0, 10616)\t0.22522950985922077\n",
      "  (0, 415)\t0.20280157594045314\n",
      "  (0, 11993)\t0.2731386994273628\n",
      "  (0, 14326)\t0.2021656129988156\n",
      "  (0, 10551)\t0.2534013186848059\n",
      "  (0, 1378)\t0.4121462714963972\n",
      "(100000, 18300)\n",
      "['#' '#1' '#2' '#define' '#ifdef' '#include' '#object' '#object method'\n",
      " '#pragma' '+']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[5])\n",
    "print(X_train_tfidf.shape)\n",
    "print(tfidf_vectorizer.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2d88b0e6-624b-4cda-b539-56f946033df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plugin'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()[11993]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da13d901-ee23-4374-b99b-5d5024523926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
